# アプリケーション実行基盤アーキテクチャ

2021-03-18 arakawa

## この文書について

* Tsurugi DB 上で SQL, NoSQL, Bulk Load 等のアプリケーションを実行するための基盤アーキテクチャについて解説

## 用語

* アプリケーション
  * ユーザからの要求を受けて、それを処理するジョブを発行する機構
  * 例えば、SQLアプリケーションは、ユーザからSQLステートメントを受け取り、SQLで記述された処理を行うジョブ (SQLジョブ) を発行する
  * トランザクションはジョブやアプリケーションをまたいで行うことができる
* ジョブ
  * SQL ステートメントなど、投入されたら処理が完了するまでユーザーが関与できない処理単位
  * ジョブは1つ以上のタスクからなり、ジョブに含まれるあらゆる操作は、基本的にタスク内で行われる
  * 1トランザクション内で複数のジョブを実行したり、逆に1ジョブ中に複数のトランザクションを実行したりできる
* ジョブスケジューラ
  * ジョブの状態 (タスクの実行状況) を管理する機構
  * ジョブの状態の公開や、ジョブのキャンセル処理を行える
  * モデルを構築する上で便利なためこの呼び方だが、実際にはスケジューリングの機能は有していない
* タスク
  * リソースの待ち合わせを(基本的に)行わずに処理を進められる単位
  * 各タスクの処理は単一のスレッド内で行い、タスク終了時に後続タスク群をタスクスケジューラに予約することで、ジョブ全体の処理を実現する
* タスクスケジューラ
  * タスクを登録すると、タスクに対して適切にワーカースレッドを割り当てて処理を進める機構
* ワーカースレッド
  * タスクを実行するための計算資源
  * タスクにワーカースレッドが割り当てられた場合にのみ、タスクに割り当てられた処理を進めることができる
  * ワーカースレッドがタスクの処理を開始すると、そのスレッドはタスクの完了まで他のタスクが割り当てられることはない

## アプリケーション

本項では、Tsurugi DB 内でアプリケーションを実行する方式の概要について解説する。

### アプリケーションの種類

現在、以下のようなアプリケーションを想定している。

* SQL 実行エンジン
  * SQL を受け取って、その記述内容を実行するアプリケーション
* Bulk Data Loader
  * バルクデータを受け取って、ストレージに格納するアプリケーション
  * または、ストレージ上のバルクデータを抽出するアプリケーション
* Native (NoSQL) Application
  * ストレージを直接操作するカスタムアプリケーション

![アプリケーション配置図](20210318-jobsch/diagrams-p1.png)

### インターフェース

アプリケーションはいずれかのインターフェースに紐つけられており、アプリケーションへの要求はそのインターフェースを介して行われる。

現在は以下のようなインターフェースを想定している。

* IPC インターフェース
  * 概要
    * 同一ノード内からIPCを介してアプリケーションを操作する
  * 制御方式
    * 制御は MQ でラップした IPC で行い、データは共有メモリやローカルファイルを介して行う
  * 想定するクライアント
    * UDF サービス
    * バックドアサービス
    * PostgreSQL FDW
  * 接続先のアプリケーション
    * SQL 実行エンジン
    * Bulk Data Loader
* REST インターフェース
  * 概要
    * HTTP(s) 上の通信を利用してアプリケーションを操作する
  * 制御方式
    * 制御は request URL で行い、データは request/response body で行う
    * request URL はあらかじめアプリケーションへのルーティングを登録しておく必要がある
  * 想定するクライアント
    * 外部システム
  * 接続先のアプリケーション
    * SQL 実行エンジン (optional)
    * Bulk Data Loader
    * Native Application
* PostgreSQL FDW インターフェース
  * 概要
    * フロントエンドの PostgreSQL から、 Foreign Data Wrapper (FDW) 経由でアプリケーションを操作する
  * 想定するクライアント
    * PostgreSQL (Frontend)
  * 接続先のアプリケーション
    * SQL 実行エンジン
    * Bulk Data Loader
  * 備考
    * IPC インターフェースに統一したいが、pg 特有機能を考えると微妙なライン

### トランザクション管理

* それぞれのアプリケーションは、いずれかのトランザクション内で処理を実行することになる
  * SQL 実行エンジン
    * SQL 内でトランザクションの開始、終了を宣言できる
    * また、実行中のトランザクションにアタッチしてその内部で SQL を実行できる
  * Bulk Data Loader
    * リクエストごとに、トランザクションの開始、継続、終了を指定できる
  * Native Application
    * アプリケーションごとに異なる
      * 外部からトランザクションの制御を指示する必要があるもの
      * 逆に内部でトランザクションを制御するため、外部からの指定が行えないもの
* インターフェースごとに、トランザクションの開始、継続、終了の指定方法が異なる
  * IPC インターフェース・REST インターフェース
    * リクエストヘッダで、トランザクションの開始・継続・終了・未指定の指定を行う
    * レスポンスヘッダで、トランザクションの状態、およびトランザクションハンドルが返される
    * トランザクションは基本的に時限付きのリースで、リース期限を更新しないと関連するトランザクションが破棄される
  * PostgreSQL FDW インターフェース
    * Tsurugi DB の API を利用し、直接操作する

## タスクスケジューラ

本項では、タスクの実行を制御する、タスクスケジューラについて解説する

### コンセプト

* すべてはタスク
  * 「ジョブ」はタスク管理上のモデルとしてのみ存在し、実態はタスクの集合
  * 「ジョブを発行する」とはジョブのブートストラップタスクをタスクスケジューラに投入する、ということ
  * 複数のタスクからなるジョブは、各タスクが後続タスク (子タスク) 群を投入することで実現
  * ジョブの構造が複雑になる代わりに、スループットが向上する
* 高ロードアベレージ下において、スループットが最大になるようにする
  * スループットを向上させるため、ジョブの fairness や latency を一部犠牲にする
  * work stealing アルゴリズムを採用し、それぞれのジョブが単一のスレッドで処理を行う際の性能を最大化しつつ、タスク並列化にも対応する
  * 各タスクは(明かな)コンテキストスイッチを行わずに処理を行える単位に分解し、計算資源を有効活用する
    * タスク間コミュニケーションは、基本的にタスク(親タスク)とそこから生成した子タスク間のみで行い、タスク間の待ち合わせが発生しないようにする
    * 子タスクは基本的に親タスクと同じワーカースレッドで実行することを基本とするが、他のスレッドが手すきの場合のみ並列化を企図して複数のスレッドで実行する
    * 子タスクが親タスクの出力を待合せる場合、子タスクは条件付きタスクとして自身を再登録し、計算資源を手放す
    * 各タスクはノンプリエンプティブであり、スレッドを他のタスクに回したいときには、自身のタスクの処理を中断し、残りの処理を別タスクとして登録したうえで、現在のタスクを終了すればいい
  * ワーカースレッドに割り当てるタスクが存在しない場合、スレッドは不活性化 (deactivate) 状態に入り、再び活性化 (activate) 状態に移行するにはペナルティが掛かる
    * すべてのスレッドをビジーループにするとそれはそれで面倒なため、不活性化状態から活性化状態に移行するためには誰かに起こしてもらう必要がある
    * 高ロードアベレージ時のスループットを稼ぐため、不活性化状態のスレッドを積極的には活性化状態には持って行かず、専用の機構によって活性化させられるのを待合せる必要がある
* 分散システム化は後で考える
  * 高スループットに寄せたために、分散システム対応と相性が悪い点がいくつかある
  * 必要に応じて見直すことにする

### タスクスケジューリングの流れ

* 前提
  * タスクスケジューラには、1個以上のワーカースレッドが登録されている
  * 各ワーカースレッドは、タスクを格納するためのローカルタスクキュー (LTQ) を有する
* ワーカースレッドの処理概要
  * (a) アプリケーションがユーザーから要求を受け取ると、アプリケーションはタスクスケジューラに対応するタスクを登録する
  * (b) タスクスケジューラは、アプリケーションから受け取ったタスクを任意のワーカースレッド上のLTQに追加する
    * この時、対象のワーカースレッドが不活性化状態であれば、活性化状態に移行させる
  * (c) ワーカースレッドは、自身の LTQ から先頭のタスクを取り出す
    * (c-1) タスクが存在した場合
      * (c-1-1) そのタスクに現在のワーカースレッドを割り当て、処理を行う
      * (c-1-2) (c) に移行
    * (c-2) タスクが存在しない場合、(d) に移行
  * (d) ワーカースレッドは、タスクスケジューラ内の各ワーカースレッドについて、LTQからタスクを取り出す (stealing)
    * (d-1) タスクが存在した場合
      * (d-1-1) そのタスクに現在のワーカースレッドを割り当て、処理を行う
      * (d-1-2) (c) に移行
    * (d-2) タスクが存在しない場合、(e) に移行
  * (e) ワーカースレッドは、タスクスケジューラに「タスク配布モード」に移行できるか問い合わせる
    * (e-1) タスクスケジューラ内にタスク配布モードのスレッドがいない場合、後述の「タスク配布モード」に移行
    * (e-2) タスクスケジューラ内にタスク配布モードのスレッドが既に存在する場合、(f) に移行
  * (f) ワーカースレッドは、不活性化モードに移行する
    * (f-1) 再び活性化モードに移行した場合、(c) に移行する
* ジョブの流れ
  * アプリケーションは、ユーザーの要求に対してタスクをタスクスケジューラに投入する
    * このタスクを、便宜上「ブートストラップタスク」とよぶ
    * ジョブの状態変数の初期化など、ジョブの処理に必要な準備を行う
    * 子タスクを投入する際、上記で作成した情報を子タスクに引き回すことで、ジョブ全体の状態をタスク間で共有する
  * 各タスクは、連鎖的に子タスクを作成することでジョブ全体の処理を行う
    * タスク間の待ち合わせを行いたい場合、共有しているジョブの状態を見てどれか一つのタスクだけが待ち合わせ後の子タスクを投入することで実現できる
      * このあたりの複雑さがプログラムを難しくする反面、うまくやればコンテキストスイッチを排除して処理を行える
    * 子タスクを現在のワーカースレッドの LTQ に追加することで、ジョブ内で複数のタスクに分割した処理を行える
      * このとき、処理の途中で子タスクを投入したり、複数の子タスクを投入することで、並列化処理を行う余地が生まれる
  * TBD
    * ジョブごとに予測タスク数と、現在の消化率をモニターすることで、 progress monitoring がある程度できる
      * そのような情報は、名前負けしている「ジョブスケジューラ」に役割を担ってもらう予定
    * memory quota 等はどう実現する？

### タスク配布モード (task distribution mode)

* 概要
  * タスク配布モードは、ワーカースレッドが自身のLTQにタスクが存在しない際に、 busy loop で他のワーカースレッドからタスクを奪い、非活性化状態のワーカースレッドに配布する操作を行うモード
  * 通常の work stealing と異なり、steal したタスクを可能な限り他の非活性化スレッドに割り当てる操作
    * LTQ からのフェッチや、通常の work stealing が成功する限りこのモードに移行しない
    * そのため、高ロードアベレージ下では基本的にこのモードにならない
  * タスク配布モードに移行するワーカースレッドは、スレッドプール内で高々1つ
* タスク配布モードの処理概要 (「タスクスケジューリングの流れ」の e-1 から移行)
  * (i) ワーカースレッドは、タスクスケジューラ内の各ワーカースレッドについて、LTQからタスクを取り出す (stealing)
    * (i-1) タスクが存在する場合、 (ii) に移行
    * (i-2) 活性化状態のワーカースレッドが1つ以上存在し、かついずれにもタスクが存在しない場合、 (i) に移行
      * このとき、 busy loop になるので `mm_pause` を挟む等の考慮が必要か
    * (i-3) 活性化状態のワーカースレッドが存在しない場合
      * 自身のタスク配布モードを解除
      * 自身を非活性化モードへ移行する
  * (ii) ワーカースレッドは、タスクスケジューラ内の各ワーカースレッドについて、非活性化状態のものを選択
    * (ii-1) 非活性化状態のワーカースレッドが存在する場合
      * 当該ワーカースレッドのLTQに (i) で取り出したタスクを投入し、当該スレッドを活性化状態へ移行させる
      * (i) へ移行
    * (ii-2) 非活性化のワーカースレッドが存在しない場合
      * 自身のタスク配布モードを解除
      * 自身のワーカースレッドのLTQに (i) で取り出したタスクを投入し、「タスクスケジューリングの流れ」の (c) へ移行 (通常のスケジューリングに戻る)
* 備考
  * (i-3) はタイミングを間違えると活性化状態のワーカースレッドがありつつ、タスク配布モードがいない、という状況を作れてしまう
    * これを避けるには、活性化状態のフラグと、タスク配布モード状態のフラグを同一の mutex で管理する等が必要
    * これらの推移はそれほど高頻度ではないのと、これらの状態遷移を行う状況は高負荷状態ではないため、普通に mutex で十分ではないかと考える
  * (ii-2) において、タイミングの問題でタスク配布モードが誰も存在せず、かつ非活性化状態のワーカースレッドが存在する状態になりうる
    * が、誰かしらのLTQが空になった際に再度タスク配布モードに移行するはずなので、ここでは考慮しない
    * 多発するようであれば、活性化スレッドがたまにタスクスケジューラに問い合わせ、上記の状態を解消するようにすればよい

### 条件付きタスク, 保留タスク (conditional tasks, pending tasks)

* 概要
  * 条件付きタスクは、子タスクが親タスクの出力を continuously に処理する際に、親タスクの準備が整うまでその処理を一時的に保留する機構である
  * 子タスクを fine grained に設計することで不要となる可能性はあるが、当初計画にもあったものなので実現方法を示す
* 条件付きタスクの取り扱い
  * LTQ から取り出したタスクが条件付きタスクであり、かつその条件を満足していない場合、LTQ に戻してタスクを取り出さなかったものとして扱う
    * そのため、LTQ 上のタスクがすべて保留状態であった場合、実質的に LTQ からタスクを取り出せないことと同義になる
  * タスクが、自身を保留状態に移行したい場合、自身のタスクのコピーを保留状態に設定して再登録することで実現する
* 備考
  * 実際の実現方法は、以下のような例が考えられる
    * LTQ とは別に保留タスクリスト (pending tasks list, PTL) を用意する
    * LTQ から条件付きタスクを取り出し、条件を満たしていた場合にはそのまま実行するが、そうでない場合は PTL の末尾にタスクを登録する
    * 別の機会に LTQ からタスクを取り出す際、LTQ より前に PTL を前方から走査し、条件を満たしたタスクがあれば、それを PTL から取り出してタスクを実行する
    * PTL に条件を満たすタスクが一つもない場合、通常と同様に LTQ のタスクを処理する
  * 上記の例は stealing と微妙に相性が悪いので、検討は必要
    * PTL に入ってしまうと steal するのが難しく、再配分の仕組みが別途必要になるかもしれない
  * 上記の例の微妙な利点は、PTLに保留タスクが残ったままLTQを空にできるので、非活性化状態に移行できる点である
    * この場合、保留タスクはいつまでたっても実行されなくなるため、タスク配布の仕組みが保留タスクのチェックを行う等の仕組みが必要となる
  * 定期的に PTQ から LTQ に移管してやることで、stealing の恩恵を受けられるようになるかもしれない
    * たとえば、タスクごとに保留カウンタを持っていて、カウンタが一定に達するごとに LTQ 送りにするなど
    * PTQ に関する contention を避けるには、いずれにしろ PTQ のオーナーのみが一貫して PTQ->LTQ への移管ができるようにしておくとよさそう

### 移動制限タスク (sticky task)

* 概要
  * 移動制限タスクは、特定のワーカースレッド上に限り実行可能なタスクである
  * 主にストレージ層の同時実行数の制約により、同一トランザクションからの並行操作が許容されていない場合などの利用を想定している
    * ややナイーブな実現方法のため、これでいいのかは実験が必要になる
* 移動制限タスクの取り扱い
  * stealing 操作を行う際、別のワーカースレッドの LTQ から取り出したタスクが移動制御タスクであった場合、LTQ に戻してタスクを取り出さなかったものとして扱う
* 備考
  * 実際の実現方法は、 LTQ とは別に移動制限タスクキューを用意するのがよさそう
    * LTQ上のタスクよりも、当該タスクキューからのタスクを優先することで、リソース管理的にもプラスに働くのではないか
  * 特定のスレッドに対し、複数のジョブが集中して移動制限タスクを投入するとスループットが低下する
    * どのワーカースレッドのキューに投入するかは、上位のタスクスケジューラーにお任せしたほうが無難か
    * トランザクションごとに固定されればいいので、トランザクションハンドルと移動制限タスクを渡して、スケジューラーが適切な対象を割り当てるイメージ
  * 当初予定の mutex のような挙動をするレーン制限は、条件付きタスクの仕組みを利用することで実現可能
    * 他の排他的タスクが稼働中は、自身を保留状態に移行する

### タスク実行レーン (task processing lanes)

* 概要
  * タスク実行レーンは、特定のタスク群を同時実行数を限った状態で処理するための機構である
    * ワーカースレッドは、タスク実行レーンの「占有権」を保有するタスクを作成し、そのタスクの中でのみレーン内の処理が行える
    * このタスクをほかのタスクと同様に取り扱うことで、同時実行数を制御しつつ、適切なスケジューリングが行える
  * 主にストレージ層の同時実行制約であり、「移動制限タスク」と同じ問題を解決しようとしている
    * そのため、どちらか一つの機構でよさそう
* 前提
  * トランザクションごとに、対応するタスク実行レーン (以下、単に「レーン」) が存在する
  * 各レーンは、レーン内で実行すべきタスクを登録可能な、個別のタスクキューを持っている
  * 各レーンには、タスクによる占有権が存在し、レーンは占有権の払い出し状況を把握している
* 動作の流れ
  * (A) 各アプリケーションのタスクは、トランザクション上のI/O操作を伴うタスクを登録する際、LTQではなく対応するレーンに登録する
    * (A-1) タスクを受け取ったレーンは、レーン内のタスクキューに当該タスクを登録する
  * (B) レーンに当該タスクを登録したタスクは、レーンの占有権の獲得を試みる
    * (B-1) 占有権を獲得できた場合、占有権を行使可能なレーンタスクを現在のスレッドの LTQ に追加し、タスクの処理を続行する
    * (B-2) 占有権を獲得できなかった場合、何もせずタスクの処理を続行する
  * (C) 各スレッドは、LTQ から レーンタスクを取り出すと、次のように処理する
    * (C-1) レーン内のキューから先頭のタスクを取り出し、タスクを実行する
    * (C-2) (C-1) を規定の回数繰り返す
    * (C-3) レーンの占有権の返却を **試み**、拒否された場合には (C-1) に戻るか、またはレーンタスクを LTQ に再登録する
      * (C-3-1) レーンは、占有権を返却する際に、タスクキューにタスクが残っている場合には、返却を拒否する
      * (C-3-2) そうでない場合、レーンは占有権を返却する
    * (C-4) 「タスクスケジューリングの流れ」の (c) へ移行 (通常のスケジューリングに戻る)
* 備考
  * 占有権の返却に失敗する場合、レーン内でまだやるべきことが残っていることを表している
    * 他のタスクによって新たなタスクがレーンに追加された場合等
      * これを避けるために、レーンへのタスク登録、占有権の獲得・返却は mutex 内でやってもよいかもしれない
    * レーンタスクは、レーンへの操作と同時にタスクとしてスケジュールされない限り、誰も実行しえない
    * ちょっとこのあたり怖い仕様なので、タスク配布モードがたまに監視してもよいかもしれない
  * レーン方式のメリットは、レーン自体をただのタスクとして取り扱うため、 stealing の仕組みで自動的にリバランスを行える点である
    * これは、移動制限タスクよりも優れている可能性がある
    * ただし、高ロードアベレージ下では移動制限タスクよりもやや処理が複雑なため、性能面では劣りそう
  * 注意すべき点として、占有権を獲得したままレーンタスクが行方不明になるとストールする
    * 占有権の獲得からLTQにレーンタスクを投入し終わるまでに不具合があると、その状況は起こりえる
    * または、レーンにタスク登録して誰も占有権を獲得しなくても同様 (こちらはプログラミングモデルで対応できそう)
  * このあたりは手法を比較し、適切な方法を選択したい

### アプリケーション出力

* 概要
  * 多くのアプリケーション(AP)は、リクエストに対して何らかの結果を返す
* 前提
* 動作の流れ
  * (1) AP実行基盤のインターフェース (I/F) がリクエストを受け取ると、対応するアプリケーションに対して、リクエストの内容と出力チャネルを渡す
  * (2) AP実行基盤は、I/Fから渡されたリクエストを解析し、ブートストラップタスクをタスクスケジューラに投入する。このとき、ブートストラップタスクに出力チャネルを引き渡す
  * (3) ブートストラップタスク、および各派生タスクは、後続のタスクを投入する際、出力チャネルを引き継ぐ
  * (4) アプリケーションの結果を出力するチャネルは、次の手順で出力を行う
    * (4-1) 出力チャネルから出力バッファを取得する (acquire)
    * (4-2) 出力バッファに次々と結果を書き出す
    * (4-3) 出力チャネルに出力バッファをステージする (stage)
  * (5) リクエストに対し、すべての出力が完了したら、タスクは出力チャネルにステータスを設定する
  * (6) I/Fは出力チャネルにステータスが設定されると、リクエストの送り主にステータスと、(必要であれば)ステージされた出力バッファの内容を通知する
* 備考
  * ステータスが出力を伴わない種類 (エラー等) である場合などには、ステージされた出力バッファの内容を破棄する
  * ステータスが設定されるより前に、ステージされたバッファを断続的に送ることで、完了前から断続的に結果を受け取ることも可能
  * 提供されるバッファを共有メモリ等に割り当てることで、I/Fでのコピー回数を減らしたい
  * acquire のバッファをリクエストごとに複数持つことで、並列出力が可能
  * 逆に、並列出力を抑制したい場合、タスク実行レーンや移動制限タスク等の仕組みを利用するのがよさそう
  * 入力チャネルは、単なるペイロードデータへのポインタになる予定なので説明は割愛
  * HTTP I/F の場合、AP実行基盤の特性も考えると、レスポンスをブロックする方式はあまりよろしくなさそう
    * たとえば、即座に `202 Accepted` と結果を確認するためのトークン (またはURL) を返しつつ、裏では出力をファイルに書き出し、出力が完了したらトークン経由で出力を提供するなど
    * ただ、メモリ管理やファイル管理が少々めんどくさそう
    * また、セッションの管理に近いことをしなければならず、その情報をどこに保存しようか

## その他

### 要確認項目

* タスクスケジューラのアルゴリズムが未検証なので、期待に沿った性能が出るか確認
* 条件付きタスクのスレッドアサイメントがちょっと効率的かどうか不明なので、forward 系で変な性能劣化がないか確認

### Asakusa Framework 連携メモ

* m3bp をターゲットに Tsurugi DB と連携
* transactional Dump -> m3bp -> transactional Load で transactional batch を実現
* Tsurugi DB と同一ノードで動かすのはリソースの奪い合いになってによくないので、別ノードでの稼働を前提にする
* 別ノードなので App I/F は REST モード

### アイデアメモ

* steal するタスクは1つずつではなく、Go に倣ってキュー上の半数のほうが steal 頻度的にも、cache locality 的にも有利かもしれない
* socket locality について考慮していないので、steal 対象の優先度や、タスク配布時の配布先の優先度等は考慮したほうがいいかもしれない
* タスク配布モードが無駄に複雑な可能性があるため、専用のスレッドを作ったほうが結果的に速くなる可能性がある
  * 特に IPC I/F の MQ ポーリングあたりと共存させればそれで十分になるかも
* 高負荷時に極端に大きなタスクがあると、同じLTQにいる別のタスクに処理が周ってこない
  * 最低限の preemption の仕組みをつける？
    * 監視スレッドが巡回しながらカウントアップして、タスクが切り替わったらカウントを0にする
    * カウントが一定以上なら、タスクは自身で現在の処理を中断し、残りの処理を別タスクとして登録しなおす
  * このカウントアップの仕組み自体、ハングアップ検出に便利かもしれないので要検討
* TBD: I/Oタスクは当該トランザクションのトランザクションスロットに一時的に登録
  * トランザクションスロットから誰かが順次取り出して実行
  * トランザクションスロット自体は単なるタスク
  * 空になったらI/Oキューに預ける？

### トランザクションスロット

TBD: いまいちなので廃案だが、分散システム等を考えると流用できそうなので残しておく

* 前提
  * トランザクションごとに、対応するトランザクションスロットが存在する
  * トランザクションスロットごとに占有権を表すトランザクションチケットが存在し、各トランザクションスロットはチケットの払い出し状況を把握している
  * システム全体に I/O キュー (GIOQ) が存在する
* 動作の流れ
  * (A) 各アプリケーションのタスクは、トランザクション上のI/O操作を伴うタスクを登録する際、LTQではなく対応するトランザクションスロットに登録する
    * (A-1) トランザクションチケットを払い出し済みの場合、(B) へ移行
    * (A-2) トランザクションチケットを払い出し済みでない場合
      * (A-2-1) GIOQ にトランザクションチケットを登録し、チケットを払い出し済みに変更する
      * (A-2-2) (B) へ移行
  * (B) 各スレッドは、LTQからタスクを取り出す前に、GIOQからトランザクションチケットを取り出す
    * (B-1) GIOQ からトランザクションチケットを取り出せた場合、(C) へ移行する
    * (B-2) GIOQ からトランザクションチケットを取り出せなかった場合、「タスクスケジューリングの流れ」の (c) へ移行する
  * (C) 当該スレッドは、トランザクションチケットを利用し、トランザクションスロット内のタスクを実行する
    * このとき、タスクをいくつ実行してもよい
  * (D) 当該スレッドは、トランザクションスロットのタスク実行が完了したら、トランザクションチケットを対応するトランザクションスロットに返却する
  * (E) 当該トランザクションスロットは、トランザクションチケットを未払い出しの状態にする
  * (F) 「タスクスケジューリングの流れ」の (c) へ移行する
* 備考
  * GIOQ がグローバルに存在するため、そこに毎回問い合わせに行くのは多少不安が残る
    * https://moodycamel.com/blog/2014/a-fast-general-purpose-lock-free-queue-for-c++.htm#benchmarks 等を見ても、グローバルキューは性能がそこまでよくない
    * 大抵は空打ちになると思うが、その時のレイテンシはどうか
  * 各スレッドがグローバルキューからタスクを取り出すため、キャッシュの利用効率の低下が懸念される
  * タスク配布モードは、GIOQ も監視する必要がある

### 参考文献

* https://dl.acm.org/doi/10.1145/324133.324234
* https://rakyll.org/scheduler/
