# IPC通信ベンチマーク

2023-04-11 ohkubo (NT)

## ベンチマーク概要
本ベンチマークは、IPC通信の基本的な通信処理について、ベーススコアを調査するためのものである。
２つの主要経路を調査するため、以下のベンチマークがある。

- ``reqres_loop``: request/response経路について、往復通信などの通信性能を調査する。
- ``data_channel_write``: resultsetの通信経路であるdata_channel経路について、送信処理の性能を調査する。

## ビルド方法
- ``-DBUILD_BENCHMARK=ON`` 指定でビルドする。デフォルトで``ON``になっている。
  - ビルド不要の場合は、``-DBUILD_BENCHMARK=OFF`` とする。
  - tateyama 自体のビルド方法は、tateyama/README.md などを参照。

## ベンチマーク実行環境について
- ベンチマークプログラムの起動ディレクトリはどこでも良い。
- サーバーの起動環境や設定ファイルの準備は不要である。
- サーバーやクライアントを前もって起動する必要はない。

## 実行方法
### reqres_loop
#### 概要
IPCのrequest/response経路について、スループットなどを計測する。
サーバーはいわゆるエコーサーバーとし、クライアントが送信した要求内容をそのまま応答として返信する。
生成するセッション数やメッセージサイズ、送信回数などは指定する。

実行結果は、CSV形式で標準出力に表示される。

#### 実行方法
```
　　　build/bench/reqres_loop/reqres_loop [{mt|mp} nsession msg_len nloop] [sync|async|nores] 
```

#### 実行オプション
- {mt|mp}：マルチスレッドクライアントかマルチプロセスクライアントかを指定する
   - ``mt``：1つのクライアントプロセスがセッション数分のスレッドをすべて生成する。
   - ``mp``：1プロセス1セッションとし、クライアントプロセスをセッション数個生成する。
- *nsession*：生成するセッション数
- *msg_len*：送信するメッセージサイズ（バイト）。メッセージヘッダサイズは含まない。
- *nloop*：通信方法が
   - ``sync``, ``async``のとき：メッセージの往復回数（メッセージの総送信回数＝2 * nloop回）
   - ``nores``のとき：要求メッセージの送信回数（メッセージの総送信回数＝nloop回）
- {sync|async|nores}：通信方法をいずれかで指定する。無指定の場合は、``sync``とみなす。
  - ``sync``：クライアントは、要求送信と応答受信を同期的に行う。
   クライアントは要求送信をし、サーバーからの応答を受信する。これを単純に繰り返す。
   サーバーでの要求待ちとクライアントでの応答待ちがあるため、CPUの実利用率は50%となる。
    1セッションあたりの使用スレッド数は、サーバー・クライアントに各一つの計2つとなる。
  - ``async``：クライアントは、要求送信と応答受信を非同期に行う。
	 つまり、一つのクライアントは要求送信スレッドと応答受信スレッドの2つを持ち、それぞれが独立して処理を進める。
	 同期的な送受信と違い、サーバーでの要求待ちやクライアントでの応答待ちがないため、スループットが向上する。
	 ただし、1セッションあたりの使用スレッド数が3つになるため、CPUの物理コア数が少ない環境では、性能が頭打ちになるのが早くなる。
  - ``nores``：クライアントはサーバーに要求送信する。サーバーは応答を送信しない。
	 request経路のみの通信速度を調査するため、応答の送受信を行わない。
	 クライアントは待ちなく要求を送信し、サーバーはそれを受信し、読み捨てる。
	 1セッションあたりの使用スレッド数は、``sync``と同じ、2つとなる。
	 
#### 起動例
- ```.../reqres_loop mt 4 128 100000```
   - マルチスレッドクライアントで4セッション生成し、128バイトのメッセージを10万回往復する。
   - 通信方法は、無指定のなので``sync``になる。
- ```.../reqres_loop mt 4 256 100000 async```
   - マルチスレッドクライアントで4セッション生成し、256バイトのメッセージを10万回往復する。
   - 通信方法は、``async``とする。
- ```.../reqres_loop mp 8 1024 100000 nores```
   - マルチプロセスクライアントで8セッション生成し、1024バイトのメッセージを10万回送信する。
   - 通信方法は、``nores``とする。
   
- ```.../reqres_loop help``` とすると、簡易ヘルプが表示される。
- オプション無し（もしくは通信方法指定のみ）で起動すると、様々なオプション設定でのベンチマークを順に繰り返す。
  - これは開発者向けのベンチマーク兼プロファイリング機能である。利用は推奨しない。
  - 実行されるオプション内容はソースコードを参照すること。
  - 実行中にリソース利用情報なども取得し、ベンチマーク終了時にまとめて表示する。

#### 出力形式
1行のCSVで標準出力に出力する。
各カラムは順に以下の値である。

- セッション数
- {mt|mp}：クライアントがマルチスレッドかマルチプロセスか
- {sync|async|nores}：通信方法
- 送信メッセージサイズ（バイト）
- サーバーの準備完了から全クライアントプロセス完了までの経過時間（秒）。精度はマイクロ秒。
- 1秒あたりの総送信メッセージ数。要求・応答の一往復を2メッセージとカウントすることに注意。
- 1秒あたりの総送信データ量（GB/秒)。

#### 実行例
```
　　　$ build/bench/reqres_loop/reqres_loop mt 4 128 100000
　　　4,mt,sync,128,100000,0.507651,1575884.389323,0.19
```

- マルチスレッドクライアントで4セッション生成し、128バイトのメッセージを10万回往復した。
- 通信方法は未指定なので``sync``となる。
- 約0.5秒かかり、1秒あたり約160万回のメッセージ送信、0.19GB/秒のスループットであった。

#### 注意点
- ベンチマークプログラムであるので、引数のチェックは厳密ではない。
- 最大セッション数や送信可能な最大メッセージサイズなどは、他のモジュールの実装依存であり、ベンチマークプログラムではチェックしていない。

### data_channel_write
#### 概要
resultsetのdata_channel経路について、スループットなどを計測する。
サーバーから指定されたサイズのデータを繰り返し送信する。
クライアントはデータを受信し、読み捨てる。
生成するセッション数やメッセージサイズ、送信回数などは指定できる。

送信は、常にサーバーからクライアントへの一方通行であるため、通信方法の指定はない。

実行結果は、CSV形式で標準出力に表示される。

#### 実行方法
```
　　　build/bench/data_channel_write/data_channel_write [{mt|mp} nsession msg_len nloop] 
```

#### 実行オプション
- 前述の``reqres_loop``の解説参考（通信方法オプションを除く）

#### 起動例
- ```.../data_channel_write mt 4 128 100000```
   - マルチスレッドクライアントで4セッション生成し、128バイトのメッセージを10万回送信する。
   
- ```.../data_channel_write help``` とすると、簡易ヘルプが表示される。
- オプション無しで起動すると、様々なオプション設定でのベンチマークを順に繰り返す。
  - これは開発者向けのベンチマーク兼プロファイリング機能である。利用は推奨しない。

#### 出力形式
1行のCSVで標準出力に出力する。
各カラムは順に以下の値である。

- セッション数
- {mt|mp}：クライアントがマルチスレッドかマルチプロセスか
- 送信メッセージサイズ（バイト）
- サーバーの準備完了から全クライアントプロセス完了までの経過時間（秒）。精度はマイクロ秒。
- 1秒あたりの総送信メッセージ数。
- 1秒あたりの総送信データ量（GB/秒)。

前述の``reqres_loop``での通信方法のカラムがないだけで、あとは同じである。

#### 実行例
```
　　　 $ build/bench/data_channel_write/data_channel_write mt 4 1024 1000000
　　　 4,mt,1024,1000000,0.556,7192080.3,6.86
```

- マルチスレッドクライアントで4セッション生成し、1024バイトのメッセージを100万回送信した。
- 約0.6秒かかり、1秒あたり約719万回のメッセージ送信、6.86GB/秒のスループットであった。

#### 注意点
- ベンチマークプログラムであるので、引数のチェックは厳密ではない。
- 最大セッション数や送信可能な最大メッセージサイズなどは、他のモジュールの実装依存であり、本プログラムではチェックしていない。
- 送信可能な最大メッセージサイズは、現時点では、64*1024 - 4 = 65532バイトである。本プログラムではチェックしていない。

## 関連ファイル
サーバー稼働に必要なファイル・ディレクトリが自動的にランダムな名前で作成される。
通常はベンチマークの終了時にすべて削除するが、強制停止した場合はそのまま残るので、適宜削除すること。

- ワーキングディレクトリ
  - /tmp/tateyama-test-*xxxxxxxx*/
- 共有メモリファイル：/dev/shm/配下に、起動ごと・セッションごとのファイルが生成される。
    - *xxx...xxx*.stat (*xxx...xxx*は32文字)
    - *xxxx-xxxx-xxxx-xxxx*, *xxxx-xxxx-xxxx-xxxx*-1, *xxxx-xxxx-xxxx-xxxx*-2, ...
- サーバーの起動待ち合わせ用ファイル
  - /tmp/ipc-test-*nnnnn*-*mmmmmmmm*

## メモ
### ビルド種別と性能
Releaseビルドで実行すると、Debugビルドの2〜5倍のスループットとなることがあった。
性能差は、動作環境や指定するメッセージサイズにより変化した。

### マルチスレッド・マルチプロセスのクライアントの振る舞い
理想的に並列実行されるならば、クライアントがマルチスレッドでもマルチプロセスでも、その振る舞いに大差はないはずである。
しかしながら、実際に動作させると、物理コア数以上のスレッドを稼働したときに、差が起きることがあった。
以下のように、動作環境によっても変わるようだ。

- Intel Xeon Platinum 8276 CPU（28コア、HTT無効化、4ソケットで112コア）
  - ``reqres_loop``ベンチマークで、物理コア以上のスレッドが稼働したとき（50セッション以上）、マルチプロセスのクライアントはスループットがスケールしたが、マルチスレッドのクライアントでは性能が頭打ちになった。
  - Linuxカーネルバージョン、NUMA設定などは記録しておらず、不明。
- Intel i7-13700 (16コア、24スレッド）
  - HTTを利用する（1コアに2スレッド割り当てる）とき、マルチスレッドのクライアントはマルチプロセスのクライアントより若干性能が良いことがあった。それ以外では目立った差はなかった。
  - Linux 5.15.0-69-genericを利用したが、これは当該CPU(Raptor Lake)に正式対応していないため、性能が引き出せていなかった可能性がある。

新しい動作環境では両方で実行し、振る舞いに差がないか調査するのが望ましい。
通常はどちらか片方で実行すれば十分である。

参考文献：

- 『試して理解 Linuxのしくみ　増補改訂版』（技術評論社, 2022）pp.125-127
  - ユーザースレッドとカーネルスレッドの情報管理やスケジューリングの違いについての解説がある。
- 『LINUX プログラムインターフェース』（オライリー, 2012）pp.729-738
  - 少し古いが、Linuxのスレッドモデルの種類と歴史、スレッドライブラリ種別の判別方法などの解説がある。
- 『LINUX システムプログラミング』（オライリー, 2008）pp.170
  - 少し古いが、Linuxのスレッドの解説がある。

### ベンチマークの関連ツール
- top
  - 起動後、``1`` と ``t`` を手入力すると、論理コアごとの使用率が（テキスト）棒グラフで表示される。
- pidstat
  - ``pidstat 1`` などとするど、1秒毎にプロセスの動作状況が``top``とほぼ同様に表示される。
  - CPUの欄で、そのプロセスがどのコアで動作しているかがわかる。
- glances
  - ``top`` の機能拡張版のようなもの。 
  - 起動後、``s`` を入力すると、CPUのコアごとの温度などがわかる。
- turbostat
  - CPUのコアごとの詳細情報（実動作クロック、ビジー率、IPC、割り込み回数、温度など）を定期的に表示する。
- taskset
  - プロセスが稼働するCPUコアを簡易に指定できる。
  - コアの排他利用はできない。一般ユーザーで利用可能。
  - ``taskset --cpu-list 0-14:2`` *program* などとすれば、0, 2, 4, .., 14の8コアで稼働する。
- cgroup の cpuset 指定
  - /sys/fs/cgroup/cpuset 配下に任意の名前のフォルダを作成し、利用するCPUコアを指定できる。
  - コアの排他利用の指定ができる。rootで実行。
  - HTTのある物理コアの片方の論理コアを排他利用指定しても、もう片方の論理コアは排他利用とならないことに注意。
  - ``echo $$ > /sys/fs/cgroup/cpuset/``*name*``/tasks`` とすることで、現在のシェル（のPID=$$）が動作指定対象プロセスになる。
    そのシェルから起動したプロセス（と子プロセス）も動作指定対象となる。
  - ENOSPCエラーが発生した場合は以下を参照
     - https://serverfault.com/questions/579555/cgroup-no-space-left-on-device#579598

参考文献：

- 『詳解　システム・パフォーマンス　第2版』（オライリー, 2023）第6章 CPUほか

### data_channel_write実行時のメモリ利用量
現在のdata_chanel実装は、送信データをバッファに蓄えてキューにつなぎ、別の処理スレッドで順次送信することがある。
これは、送信処理完了の待ち合わせをせず、早く呼び出し元に処理を返すため、そのように実装された。
送信処理が完了すれば送信データは破棄される。

送受信の処理の状況によっては、サーバー側に送信データが一時的に大量に蓄えられることがある。
``top``などで監視すると、実利用メモリが10GB程度になることもあるが、異常ではない。

実メモリが64GBの環境で、4KBのデータを1千万回送信する（メッセージ総量は40GB）ベンチマークを実行した際、OOM killerが発動し、ベンチマークが強制終了となることがあった。
